{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb08b2b",
   "metadata": {},
   "source": [
    "## Selecting representative instances of dataset using KMeans clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e977d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fd8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef680d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_KMeans:\n",
    "    def __init__(self, dataset, num_of_clusters, initialization_method=\"kmeans++\", max_iter=300):\n",
    "        self.X = dataset\n",
    "        self.k = num_of_clusters\n",
    "        self.centroids = None\n",
    "        self.clusters = None\n",
    "        self.max_iter = max_iter\n",
    "        self.initialization_method = initialization_method\n",
    "        \n",
    "        self.initialize_centroids()\n",
    "        \n",
    "    # distance of all datas in X with each centroid\n",
    "    def distance_fn(self, X, centroids):\n",
    "        distances = np.sqrt(np.power(np.subtract(X, centroids),2).sum(axis=2))\n",
    "        return distances\n",
    "    \n",
    "    # distance of two points i & j\n",
    "    def two_points_distance_fn(self, p1, p2):\n",
    "        return np.sum((p1 - p2)**2)\n",
    "  \n",
    "    # randomly initiate k-clusters from X as centroid\n",
    "    def initialize_centroids(self):\n",
    "        if self.initialization_method == \"random\":\n",
    "            self.centroids = self.X.copy()\n",
    "            np.random.shuffle(self.centroids)\n",
    "            self.centroids = self.centroids[:self.k]\n",
    "        elif self.initialization_method == \"kmeans++\":\n",
    "            self.centroids = []\n",
    "            # add first random centroid\n",
    "            random_index = np.random.randint(0, len(self.X))\n",
    "            self.centroids.append(self.X[random_index])\n",
    "            \n",
    "            for c_id in range(self.k - 1):\n",
    "                ## initialize a list to store distances of data points from nearest centroid\n",
    "                dist = []\n",
    "                for i in range(self.X.shape[0]):\n",
    "                    point = self.X[i, :]\n",
    "                    d = sys.maxsize\n",
    "\n",
    "                    ## compute distance of point from each of the previously selected centroid and store the minimum distance\n",
    "                    for j in range(len(self.centroids)):\n",
    "                        temp_dist = self.two_points_distance_fn(point, self.centroids[j])\n",
    "                        d = min(d, temp_dist)\n",
    "                    dist.append(d)\n",
    "\n",
    "                ## select data point with maximum distance as our next centroid\n",
    "                # dist = np.asarray(dist)\n",
    "                next_centroid = self.X[np.argmax(dist), :]\n",
    "                self.centroids.append(next_centroid)\n",
    "                dist = []\n",
    "            self.centroids = np.asarray(self.centroids)\n",
    "            \n",
    "        return self.centroids\n",
    "\n",
    "    def find_closest_centroids(self):\n",
    "        \"\"\"\n",
    "        find the cluster label for each point\n",
    "        \"\"\"\n",
    "        distances = self.distance_fn(self.X, self.centroids[:, np.newaxis])\n",
    "        return np.argmin(distances, axis=0)\n",
    "    \n",
    "    def closest_farthest_of_each_cluster(self):\n",
    "        res = []\n",
    "        for k in range(self.centroids.shape[0]):\n",
    "            dists = []\n",
    "            for x in self.X[self.closest_centroids==k]:\n",
    "                dist = np.linalg.norm(self.centroids[k] - x)\n",
    "                dists.append((x, dist))\n",
    "            # sort dists and pick the first and last elements\n",
    "            dists = sorted(dists, key=lambda x: x[1])\n",
    "            res.append(dists[0][0])\n",
    "            res.append(dists[-1][0])\n",
    "        return res\n",
    "    \n",
    "    def move_centroids(self, closest_centroids):\n",
    "        # create array of new centroids\n",
    "        new_centroids = np.asarray([self.X[closest_centroids==k].mean(axis=0) for k in range(self.centroids.shape[0])])\n",
    "        return new_centroids\n",
    "    \n",
    "    def main_loop(self):\n",
    "        # while centroids do not change\n",
    "        i = 0\n",
    "        self.closest_centroids = self.find_closest_centroids()\n",
    "        self.new_centroids = self.move_centroids(self.closest_centroids)\n",
    "        while np.array_equal(self.centroids, self.new_centroids) == False:\n",
    "            if i > self.max_iter:\n",
    "                break\n",
    "            i += 1\n",
    "            self.centroids = self.new_centroids\n",
    "            self.closest_centroids = self.find_closest_centroids()\n",
    "            self.new_centroids = self.move_centroids(self.closest_centroids)\n",
    "        return self.closest_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e4c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class InstanceSelection:\n",
    "    def __init__(self, dataset, num_of_clusters, \n",
    "                 repeating_time, KMeans_Model=\"My_Kmeans\", \n",
    "                 kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300):\n",
    "        self.X = dataset\n",
    "        self.k = num_of_clusters\n",
    "        self.RT = repeating_time\n",
    "        self.new_X = []\n",
    "        \n",
    "        self.kmeans_initialization_method = kmeans_initialization_method\n",
    "        self.kmeans_max_iter = kmeans_max_iter\n",
    "        self.KMeans_Model = KMeans_Model\n",
    "        \n",
    "        self.processing_time = 0\n",
    "    \n",
    "    def My_Kmeans_main_loop(self):\n",
    "        start = time.time()\n",
    "        for i in range(self.RT):\n",
    "            kmeans = My_KMeans(self.X, self.k, initialization_method=self.kmeans_initialization_method, max_iter=self.kmeans_max_iter)\n",
    "            kmeans.main_loop()\n",
    "            # select farthest and closest instances of each cluster to the new dataset\n",
    "            closest_farthest_instances_of_clusters = kmeans.closest_farthest_of_each_cluster()\n",
    "            for instance in closest_farthest_instances_of_clusters:\n",
    "                self.new_X.append(instance)\n",
    "        self.new_X = np.asarray(self.new_X)\n",
    "        end = time.time()\n",
    "        self.processing_time = end - start\n",
    "        return self.new_X\n",
    "    \n",
    "    def Sklearn_Kmeans_main_loop(self):\n",
    "        start = time.time()\n",
    "        if self.kmeans_initialization_method == \"kmeans++\":\n",
    "            self.kmeans_initialization_method = \"k-means++\"\n",
    "        for i in range(self.RT):\n",
    "            kmeans = KMeans(n_clusters=self.k, init=self.kmeans_initialization_method, random_state=0).fit(self.X)\n",
    "            \n",
    "            # select farthest and closest instances of each cluster to the new dataset\n",
    "            for l in np.unique(kmeans.labels_):\n",
    "                cluster_datas = self.X[kmeans.labels_==l]\n",
    "                distance = []\n",
    "                for data in cluster_datas:\n",
    "                    distance.append((data, np.linalg.norm(data - kmeans.cluster_centers_[l])))\n",
    "      \n",
    "                distance = sorted(distance, key=lambda x: x[1])\n",
    "                self.new_X.append(distance[0][0])\n",
    "                self.new_X.append(distance[-1][0])\n",
    "        \n",
    "        self.new_X = np.asarray(self.new_X)\n",
    "        end = time.time()\n",
    "        self.processing_time = end - start\n",
    "        return self.new_X\n",
    "\n",
    "    def main_loop(self):\n",
    "        if self.KMeans_Model == \"My_KMeans\":\n",
    "            return self.My_Kmeans_main_loop()\n",
    "        elif self.KMeans_Model == \"sklearn_KMeans\":\n",
    "            return self.Sklearn_Kmeans_main_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25993a93",
   "metadata": {},
   "source": [
    "## Outlier Detection based on KSE statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898f37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "class OutlierDetectionReductionKSE:\n",
    "    def __init__(self, x_train, repeating_time, KS_type=\"euclidean_distance\", ROT_type=\"run_1_time\"):\n",
    "        \"\"\"\n",
    "        ROT_type -> shows which kind of outloer detection is used\n",
    "        1 -> run KSE for ROT times and each time remove the biggest outlier score\n",
    "        2 -> run KSE once and remove the top ROT-instances with highest outlier score\n",
    "        \"\"\"\n",
    "        self.x_train = x_train\n",
    "        self.ROT = repeating_time\n",
    "        self.KS_type = KS_type\n",
    "        self.ROT_type =  ROT_type\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        self.outlier_detection_main_loop()\n",
    "\n",
    "        end = time.time()\n",
    "        self.processing_time = end - start\n",
    "    \n",
    "    # eulicidin distance\n",
    "    def distance_fn_2_points(self, x, p):\n",
    "        return np.linalg.norm(x - p)\n",
    "\n",
    "    def distance_fn_x_train_vs_point(self, x_train, p):\n",
    "        return np.linalg.norm(x_train - p, axis=1)      # find euclidean distance of all data in x_train with point p\n",
    "    \n",
    "    def KS_euclidean_distance(self, pj, pi):\n",
    "        distance_arr = [abs(self.distance_fn_2_points(pj, x)-self.distance_fn_2_points(pi, x)) for x in self.x_train]\n",
    "        distance_arr.sort()     # sort array ascending\n",
    "        return distance_arr[-1]\n",
    "\n",
    "    def KS_distribution_of_euclidean_distance(self, pj, pi):\n",
    "        # find ecdf of pj\n",
    "        pj_dists = self.distance_fn_x_train_vs_point(self.x_train, pj)\n",
    "        ecdf_pj_obj = ECDF(pj_dists)\n",
    "        ecdf_pj = ecdf_pj_obj(pj_dists)\n",
    "        \n",
    "        # find ecdf of pi\n",
    "        pi_dists = self.distance_fn_x_train_vs_point(self.x_train, pi)\n",
    "        ecdf_pi_obj = ECDF(pi_dists)\n",
    "        ecdf_pi = ecdf_pi_obj(pi_dists)\n",
    "            \n",
    "        # find ecdf_pj - ecdf_pi\n",
    "        total_ecdf = ecdf_pj - ecdf_pi\n",
    "        max_total_ecdf = max(total_ecdf)\n",
    "        return max_total_ecdf\n",
    "\n",
    "    def KSE(self, pj):\n",
    "        if self.KS_type == \"euclidean_distance\":\n",
    "            res = 0\n",
    "            for x in self.x_train:\n",
    "                res += self.KS_euclidean_distance(pj, x)\n",
    "            return float(res)/(len(self.x_train)-1)\n",
    "        \n",
    "        elif self.KS_type == \"distribution_of_euclidean_distance\":\n",
    "            res = 0\n",
    "            for x in self.x_train:\n",
    "                if (x==pj).all() == False:\n",
    "                    res += self.KS_distribution_of_euclidean_distance(pj, x)\n",
    "            return float(res)/(len(self.x_train)-1)\n",
    "        \n",
    "    def outlier_detection_main_loop(self):\n",
    "        # compute outlier score for all x in x_train\n",
    "        # delete the instance with biggest outlier score\n",
    "        # repeat this step for ROT times\n",
    "        \n",
    "        if self.ROT_type == \"run_ROT_times\":\n",
    "            # ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
    "            # run KSE for ROT times and remove the instance with maximum outlier score each time\n",
    "            for i in range(self.ROT):\n",
    "                outlier_scores_array = []\n",
    "                for x in self.x_train:\n",
    "                    outlier_scores_array.append(self.KSE(x))\n",
    "                remove_index = np.argmax(outlier_scores_array)\n",
    "                \n",
    "                # remove last element of outlier_scores_array from x_train\n",
    "                self.x_train = np.delete(self.x_train, remove_index, axis=0)   # delete instance on remove-index of x_train in axis row\n",
    "            # ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
    "\n",
    "        if self.ROT_type == \"run_1_time\":\n",
    "            # ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
    "            # run KSE once and remove the ROT-instances with maximum outlier score\n",
    "            \n",
    "            outlier_scores_array = []\n",
    "            for x in self.x_train:\n",
    "                outlier_scores_array.append(self.KSE(x))\n",
    "            # sort indices based on value\n",
    "            temp = np.argpartition(-np.asarray(outlier_scores_array), self.ROT)\n",
    "            max_ROT_args = temp[:self.ROT]\n",
    "\n",
    "            # remove last ROT-elements of outlier_scores_array from x_train\n",
    "            self.x_train = np.delete(self.x_train, max_ROT_args, axis=0)      # delete instance on remove-index of x_train in axis row\n",
    "            # ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n",
    "                \n",
    "        return self.x_train\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d54013",
   "metadata": {},
   "source": [
    "## Stage-1 & Stage-2\n",
    "\n",
    "* ### K-Means\n",
    "* ### Outlier Detection\n",
    "\n",
    "Now, We're going to run these two steps for each dataset for diffrent values for k, RT, ROT to see how many instances we will have at the end.\n",
    "\n",
    "> - k -> 5-30\n",
    "> \n",
    "> - RT -> 5-30\n",
    "> \n",
    "> - ROT -> 5-30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac31126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stage_1_and_2(dataset, dataset_name, k, RT, ROT, \n",
    "                  KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                  KS_type=\"euclidean_distance\", ROT_type=\"run_1_time\"):\n",
    "    \n",
    "    print(\"x_train shape: \", dataset.x_train.shape)\n",
    "    print(\"Started Stage I & II on dataset \", dataset_name, \"...\")\n",
    "                  \n",
    "    # do instance selection using K-Means\n",
    "    print(\"Instance selection started ...\")\n",
    "    instance_selection = InstanceSelection(dataset.x_train, num_of_clusters=k, repeating_time=RT, \n",
    "                                           KMeans_Model=KMeans_Model, kmeans_initialization_method=kmeans_initialization_method, \n",
    "                                           kmeans_max_iter=kmeans_max_iter)\n",
    "    new_x_train = instance_selection.main_loop()\n",
    "    print(\"Instance selection finished after \", instance_selection.processing_time, \"...\")\n",
    "    print(\"x_train shape kmeans after instance selection: \", new_x_train.shape, \"\\n\")\n",
    "    \n",
    "    \n",
    "    # remove duplicate instances\n",
    "    print(\"Started removing duplicate instances ...\", dataset_name, \"...\")\n",
    "    new_x_train = np.unique(new_x_train, axis=0)\n",
    "    print(\"x_train shape after duplication deleting: \", new_x_train.shape, \"\\n\")\n",
    "    \n",
    "    # do outlier detection and deletion using KSE\n",
    "    print(\"Outlier Detection started ...\")\n",
    "    outlier_detection = OutlierDetectionReductionKSE(new_x_train, repeating_time=ROT, KS_type=KS_type, ROT_type=ROT_type)\n",
    "    print(\"x_train shape after outlier detection: \", outlier_detection.x_train.shape)\n",
    "    print(\"Outlier Detection finished after \", outlier_detection.processing_time, \"...\\n\")\n",
    "    print(\"Finished.\")\n",
    "    print(\"~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\")\n",
    "    \n",
    "    return outlier_detection.x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f31e0c",
   "metadata": {},
   "source": [
    "## Labeling new dataset (representative datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bfec6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewDatasetHumanLabeling:\n",
    "    def __init__(self, dataset, new_x_train, output_dataset_path):\n",
    "        start = time.time()\n",
    "\n",
    "        new_y_train = self.LabelingNewDataset(dataset, new_x_train)\n",
    "        self.create_new_dataset_csv_file(new_x_train, new_y_train, output_dataset_path)\n",
    "\n",
    "        end = time.time()\n",
    "        self.processing_time = end - start\n",
    "    \n",
    "    def LabelingNewDataset(self, dataset, new_x_train):\n",
    "        new_y_train = []\n",
    "        i = 0\n",
    "        for new_x in new_x_train:\n",
    "            new_y_train.append(dataset.y_train[(dataset.x_train==new_x).all(axis=1).nonzero()[0][0]][0])\n",
    "        return new_y_train\n",
    "\n",
    "    def create_new_dataset_csv_file(self, new_x_train, new_y_train, output_dataset_path):\n",
    "        df = pd.DataFrame(new_x_train)\n",
    "        df[len(df.columns)] = new_y_train\n",
    "        # create a new csv file\n",
    "        df.to_csv(\"test.csv\", index=False)\n",
    "        \n",
    "        # remove first line of csv file which is the header of each coloumn\n",
    "        with open(\"test.csv\",'r') as f:\n",
    "            with open(output_dataset_path,'w') as f1:\n",
    "                next(f) # skip header line\n",
    "                for line in f:\n",
    "                    f1.write(line)\n",
    "        os.remove(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db03e5b",
   "metadata": {},
   "source": [
    "## Saving data test in a new csv file individualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a145d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_test(x_test, y_test, output_dataset_path):\n",
    "    df = pd.DataFrame(x_test)\n",
    "    df[len(df.columns)] = y_test\n",
    "    # create a new csv file\n",
    "    df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "    # remove first line of csv file which is the header of each coloumn\n",
    "    with open(\"test.csv\",'r') as f:\n",
    "        with open(output_dataset_path,'w') as f1:\n",
    "            next(f) # skip header line\n",
    "            for line in f:\n",
    "                f1.write(line)\n",
    "    os.remove(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859fa4c",
   "metadata": {},
   "source": [
    "## Breast-w Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d97c3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Breast-W ...\n",
      "Finished reading dataset  Breast-W ...\n"
     ]
    }
   ],
   "source": [
    "breast_w_dataset = Datasets.Breast_W_Dataset('./Datasets/breast-cancer-wisconsin.data', \"Breast-W\", \n",
    "                                    train_size=0.1, normalization_method='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "426947af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (68, 10)\n",
      "Started Stage I & II on dataset  Breast-W ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  0.27941107749938965 ...\n",
      "x_train shape kmeans after instance selection:  (216, 10) \n",
      "\n",
      "Started removing duplicate instances ... Breast-W ...\n",
      "x_train shape after duplication deleting:  (31, 10) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (21, 10)\n",
      "Outlier Detection finished after  0.18903779983520508 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(breast_w_dataset, \"Breast-W\", k=9, RT=12, ROT=10, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04e93b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_w_dataset_human_labling = NewDatasetHumanLabeling(breast_w_dataset, new_x_train, \"./NewDatasets/new_breast_w_train.data\")\n",
    "save_data_test(breast_w_dataset.x_test, breast_w_dataset.y_test, \"./NewDatasets/new_breast_w_test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615482a",
   "metadata": {},
   "source": [
    "## Messidor Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "757fc92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Messidor ...\n",
      "Finished reading dataset  Messidor ...\n"
     ]
    }
   ],
   "source": [
    "messidor = Datasets.Messidor_Dataset('./Datasets/messidor_features.arff', \"Messidor\", \n",
    "                                     train_size=0.1, normalization_method='None', \n",
    "                                     is_class_label_a_feature=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a99d53c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (115, 19)\n",
      "Started Stage I & II on dataset  Messidor ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  0.6965482234954834 ...\n",
      "x_train shape kmeans after instance selection:  (240, 19) \n",
      "\n",
      "Started removing duplicate instances ... Messidor ...\n",
      "x_train shape after duplication deleting:  (41, 19) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (36, 19)\n",
      "Outlier Detection finished after  0.45304059982299805 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36, 19)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(messidor, \"Messidor\", k=10, RT=12, ROT=5, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c19e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messidor_dataset_human_labling = NewDatasetHumanLabeling(messidor, new_x_train, \"./NewDatasets/new_messidor_train.data\")\n",
    "save_data_test(messidor.x_test, messidor.y_test, \"./NewDatasets/new_messidor_test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5798e3",
   "metadata": {},
   "source": [
    "## Car Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fedd1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Car ...\n",
      "Finished reading dataset  Car ...\n"
     ]
    }
   ],
   "source": [
    "car_dataset = Datasets.Car_Dataset('./Datasets/car.data', \"Car\", 'Class', \n",
    "                                   train_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "897dcdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (172, 6)\n",
      "Started Stage I & II on dataset  Car ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  0.558758020401001 ...\n",
      "x_train shape kmeans after instance selection:  (180, 6) \n",
      "\n",
      "Started removing duplicate instances ... Car ...\n",
      "x_train shape after duplication deleting:  (72, 6) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (62, 6)\n",
      "Outlier Detection finished after  1.3460261821746826 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(car_dataset, \"Car\", k=9, RT=10, ROT=10, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e55837b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_dataset_human_labling = NewDatasetHumanLabeling(car_dataset, new_x_train, \"./NewDatasets/new_car_train.data\")\n",
    "save_data_test(car_dataset.x_test, car_dataset.y_test, \"./NewDatasets/new_car_test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab04ee5",
   "metadata": {},
   "source": [
    "## Spambase Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f55d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Spambase ...\n",
      "Finished reading dataset  Spambase ...\n"
     ]
    }
   ],
   "source": [
    "spambase_dataset = Datasets.Spambase_Dataset('./Datasets/spambase.data', \"Spambase\", \n",
    "                                    train_size=0.1, normalization_method='None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32c81f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (459, 57)\n",
      "Started Stage I & II on dataset  Spambase ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  7.445483207702637 ...\n",
      "x_train shape kmeans after instance selection:  (390, 57) \n",
      "\n",
      "Started removing duplicate instances ... Spambase ...\n",
      "x_train shape after duplication deleting:  (44, 57) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (24, 57)\n",
      "Outlier Detection finished after  0.4269986152648926 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24, 57)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(spambase_dataset, \"Spambase\", k=13, RT=15, ROT=20, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edc10965",
   "metadata": {},
   "outputs": [],
   "source": [
    "spambase_dataset_human_labling = NewDatasetHumanLabeling(spambase_dataset, new_x_train, \"./NewDatasets/new_Spambase_train.data\")\n",
    "save_data_test(spambase_dataset.x_test, spambase_dataset.y_test, \"./NewDatasets/new_Spambase_test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45f107",
   "metadata": {},
   "source": [
    "## Coil2000 Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8980566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(196, 85)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coil2000_dataset = Datasets.Coil2000_Dataset('./Datasets/coil2000.dat', \"Coil2000\", \n",
    "                            train_size=0.02, normalization_method='None')\n",
    "coil2000_dataset.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40cbce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (196, 85)\n",
      "Started Stage I & II on dataset  Coil2000 ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  3.67228627204895 ...\n",
      "x_train shape kmeans after instance selection:  (624, 85) \n",
      "\n",
      "Started removing duplicate instances ... Coil2000 ...\n",
      "x_train shape after duplication deleting:  (51, 85) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (25, 85)\n",
      "Outlier Detection finished after  0.8941731452941895 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 85)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(coil2000_dataset, \"Coil2000\", k=26, RT=12, ROT=26, \n",
    "                            KMeans_Model=\"sklearn_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50655b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (196, 85)\n",
      "Started Stage I & II on dataset  Coil2000 ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  1.8234508037567139 ...\n",
      "x_train shape kmeans after instance selection:  (624, 85) \n",
      "\n",
      "Started removing duplicate instances ... Coil2000 ...\n",
      "x_train shape after duplication deleting:  (153, 85) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (127, 85)\n",
      "Outlier Detection finished after  9.597731590270996 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(127, 85)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(coil2000_dataset, \"Coil2000\", k=26, RT=12, ROT=26, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"random\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2ffcaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (196, 85)\n",
      "Started Stage I & II on dataset  Coil2000 ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  6.308869361877441 ...\n",
      "x_train shape kmeans after instance selection:  (624, 85) \n",
      "\n",
      "Started removing duplicate instances ... Coil2000 ...\n",
      "x_train shape after duplication deleting:  (110, 85) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (84, 85)\n",
      "Outlier Detection finished after  3.311028242111206 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84, 85)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(coil2000_dataset, \"Coil2000\", k=26, RT=12, ROT=26, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd1cd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "coil2000_dataset_human_labling = NewDatasetHumanLabeling(coil2000_dataset, new_x_train, \"./NewDatasets/new_coil2000_train.data\")\n",
    "save_data_test(coil2000_dataset.x_test, coil2000_dataset.y_test, \"./NewDatasets/new_coil2000_test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d65478c",
   "metadata": {},
   "source": [
    "## Bank Marketing Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49a1874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n"
     ]
    }
   ],
   "source": [
    "bank_dataset = Datasets.Bank_Marketing_Dataset('./Datasets/bank-full.csv', \"Bank Marketing\", 'y', \n",
    "                                      train_size=0.1, normalization_method=\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5b57cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (4521, 16)\n",
      "Started Stage I & II on dataset  Bank Marketing ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  39.0008819103241 ...\n",
      "x_train shape kmeans after instance selection:  (270, 16) \n",
      "\n",
      "Started removing duplicate instances ... Bank Marketing ...\n",
      "x_train shape after duplication deleting:  (18, 16) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (13, 16)\n",
      "Outlier Detection finished after  0.07222437858581543 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(bank_dataset, \"Bank Marketing\", k=9, RT=15, ROT=5, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a4c74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_dataset_human_labling = NewDatasetHumanLabeling(bank_dataset, new_x_train, \"./NewDatasets/new_bank_train.data\")\n",
    "save_data_test(bank_dataset.x_test, bank_dataset.y_test, \"./NewDatasets/new_bank_test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82d7bf",
   "metadata": {},
   "source": [
    "## Skin Segmentation Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc603632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n"
     ]
    }
   ],
   "source": [
    "skin_dataset = Datasets.Skin_NonSkin_Dataset('./Datasets/Skin_NonSkin.txt', \"Skin Segmentation\",\n",
    "                                             train_size=0.0005, normalization_method=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ad86596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (122, 3)\n",
      "Started Stage I & II on dataset  Skin Segmentation ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  0.19309711456298828 ...\n",
      "x_train shape kmeans after instance selection:  (150, 3) \n",
      "\n",
      "Started removing duplicate instances ... Skin Segmentation ...\n",
      "x_train shape after duplication deleting:  (24, 3) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (19, 3)\n",
      "Outlier Detection finished after  0.138962984085083 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(skin_dataset, \"Skin Segmentation\", k=5, RT=15, ROT=5, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30ab497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_dataset_human_labling = NewDatasetHumanLabeling(skin_dataset, new_x_train, \"./NewDatasets/new_skin_train.data\")\n",
    "save_data_test(skin_dataset.x_test, skin_dataset.y_test, \"./NewDatasets/new_skin_test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3569e",
   "metadata": {},
   "source": [
    "## Covertype Instance Selection + Outlier Detection + Human Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b30f3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Covertype ...\n",
      "Finished reading dataset  Covertype ...\n"
     ]
    }
   ],
   "source": [
    "covertype_dataset = Datasets.Covertype_Dataset('./Datasets/covtype.data', \"Covertype\", \n",
    "                                               train_size=0.02, normalization_method=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f727bc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (11620, 54)\n",
      "Started Stage I & II on dataset  Covertype ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  845.946711063385 ...\n",
      "x_train shape kmeans after instance selection:  (600, 54) \n",
      "\n",
      "Started removing duplicate instances ... Covertype ...\n",
      "x_train shape after duplication deleting:  (20, 54) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (15, 54)\n",
      "Outlier Detection finished after  0.09199357032775879 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 54)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(covertype_dataset, \"Covertype\", k=10, RT=30, ROT=5, \n",
    "                            KMeans_Model=\"My_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "412fefc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "covertype_dataset_human_labling = NewDatasetHumanLabeling(covertype_dataset, new_x_train, \"./NewDatasets/new_covtype_train.data\")\n",
    "save_data_test(covertype_dataset.x_test, covertype_dataset.y_test, \"./NewDatasets/new_covtype_test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d41f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (11620, 54)\n",
      "Started Stage I & II on dataset  Covertype ...\n",
      "Instance selection started ...\n",
      "Instance selection finished after  39.954097747802734 ...\n",
      "x_train shape kmeans after instance selection:  (600, 54) \n",
      "\n",
      "Started removing duplicate instances ... Covertype ...\n",
      "x_train shape after duplication deleting:  (20, 54) \n",
      "\n",
      "Outlier Detection started ...\n",
      "x_train shape after outlier detection:  (15, 54)\n",
      "Outlier Detection finished after  0.08603334426879883 ...\n",
      "\n",
      "Finished.\n",
      "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 54)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_train = Stage_1_and_2(covertype_dataset, \"Covertype\", k=10, RT=30, ROT=5, \n",
    "                            KMeans_Model=\"sklearn_KMeans\", kmeans_initialization_method=\"kmeans++\", kmeans_max_iter=300, \n",
    "                            KS_type=\"distribution_of_euclidean_distance\", ROT_type=\"run_1_time\")\n",
    "new_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "559a0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "covertype_dataset_human_labling = NewDatasetHumanLabeling(covertype_dataset, new_x_train, \"./NewDatasets/sklearn_new_covtype_train.data\")\n",
    "save_data_test(covertype_dataset.x_test, covertype_dataset.y_test, \"./NewDatasets/sklearn_new_covtype_test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64079f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
