{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "gorgeous-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "living-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from random import sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "wrong-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel():\n",
    "    def linear():\n",
    "        def result(x, y):\n",
    "            return np.dot(x.T, y)\n",
    "        return result\n",
    "    \n",
    "    def polynomial(degree, gamma=1.0, intercept=1.0):\n",
    "        def result(x,y):\n",
    "            return (gamma * (x.T @ y) + intercept) ** degree\n",
    "        return result\n",
    "\n",
    "    def gaussian(sigma = 1.):\n",
    "        def result(x,y):\n",
    "            return np.exp(-np.linalg.norm(x-y)/(2 * sigma ** 2))   \n",
    "        return result\n",
    "    \n",
    "    def sigmoid(gamma = 1., intercept = 1.):\n",
    "        def result(x,y):\n",
    "            return np.tanh(gamma * np.dot(x.T, y) + intercept)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "optional-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMkl():\n",
    "    def __init__(self, kernels, C = 1.):\n",
    "        '''\n",
    "        kernels: list of kernel functions\n",
    "        C: regularization parameter used in SVM\n",
    "        '''\n",
    "        self.kernel_functions = kernels\n",
    "        self.C = C\n",
    "    \n",
    "    def fit(self, X, y, verbose = False):\n",
    "        '''\n",
    "        X: training points\n",
    "        y: label value (1 or -1) for each training point\n",
    "        '''\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.x_train = X\n",
    "        self.y_train = y\n",
    "        self.y_matrix_outer = np.outer(self.y_train, self.y_train)\n",
    "        \n",
    "        self.get_kernel_matrices()\n",
    "        \n",
    "        self.epsilon_d = 1e-6\n",
    "        self.duality_threshold = 0.02  \n",
    "        self.epsilon_D = 1e-6\n",
    "        \n",
    "        \n",
    "        d = np.repeat(1./self.num_of_kernels, self.num_of_kernels)\n",
    "        self.model_svm = svm.SVC(C = self.C, kernel = 'precomputed')\n",
    "             \n",
    "        outer_iteration = 0\n",
    "        outer_loop_stop = False\n",
    "        \n",
    "        if verbose:\n",
    "            print('start loop ...')\n",
    "        prev_dual_gap = None\n",
    "        \n",
    "        while not outer_loop_stop:\n",
    "            # compute J and optimal alpha\n",
    "            combined_kernel_matrix = self.get_weighted_kernel_matrix(d)\n",
    "            J_d, alpha = self.get_J(combined_kernel_matrix)\n",
    "            \n",
    "            # compute dJ (gradient of J with respect to d)\n",
    "            dJ = self.get_derivative_J(alpha)\n",
    "            \n",
    "            if verbose:\n",
    "                print('outer iteration: ', outer_iteration)\n",
    "                print('J_d: ', J_d)\n",
    "                print('dJ: ', dJ)\n",
    "                print('alpha: ', alpha)\n",
    "                \n",
    "            # change outer_loop_stop            \n",
    "            duality_gap = self.get_duality_gap(J_d, alpha, dJ)\n",
    "            \n",
    "            if verbose:\n",
    "                print('duality gap: ', duality_gap)\n",
    "                print('prev dual gap: ', prev_dual_gap)\n",
    "                \n",
    "            if outer_iteration == 0:\n",
    "                prev_dual_gap = duality_gap\n",
    "            else:    \n",
    "                if np.abs(duality_gap - prev_dual_gap) < self.duality_threshold:\n",
    "                    if verbose:\n",
    "                        print('reach dual gap thershold: ', np.abs(duality_gap - prev_dual_gap))\n",
    "                    outer_loop_stop = True\n",
    "                    break\n",
    "                else:\n",
    "                    prev_dual_gap = duality_gap\n",
    "#             kkt_condition = self.get_KKT_condition(outer_iteration, dJ, d)\n",
    "#             print('kkt', kkt_condition)\n",
    "#             if kkt_condition:\n",
    "#                 outer_loop_stop = True\n",
    "#                 break\n",
    "            outer_iteration += 1    \n",
    "            \n",
    "            # set mu\n",
    "            # mu is index of the largest component in d for better numerical stability\n",
    "            mu = np.argmax(d)\n",
    "            \n",
    "            # compute D direction\n",
    "            D = self.get_D_direction(d, dJ, mu)\n",
    "            ''''''\n",
    "            D = self.l1_norm_on_sum(D, 0)\n",
    "            \n",
    "            if verbose:\n",
    "                print('mu: ', mu)\n",
    "                print('D: ', D)\n",
    "                \n",
    "            # set parameters for inner loop\n",
    "            J_prim = 0\n",
    "            d_prim = np.copy(d)\n",
    "            D_prim = np.copy(D)\n",
    "            \n",
    "            inner_iteration = 0\n",
    "            # inner loop : descent direction update\n",
    "            # J_prim < J_d - epsilon\n",
    "            # and  J_d - J_prim > self.J_thershold\n",
    "            while J_prim < J_d:\n",
    "                inner_iteration += 1\n",
    "                \n",
    "                if verbose:\n",
    "                    print('inner iteration: ', inner_iteration)\n",
    "                \n",
    "                # set parameters\n",
    "                d = np.copy(d_prim)\n",
    "                D = np.copy(D_prim)\n",
    "                \n",
    "                # check d\n",
    "                flag = False\n",
    "                for i in d:\n",
    "                    if i == 1:\n",
    "                        flag = True\n",
    "                        break\n",
    "                if flag == True:\n",
    "                    break\n",
    "                  \n",
    "                # find gamma_max, v\n",
    "                gamma_max, v = self.get_gamma_max_and_index(d, D)\n",
    "                if gamma_max == None:\n",
    "                    gamma_max, v = 0, 0\n",
    "                    \n",
    "                if verbose:\n",
    "                    print('d: ', d)\n",
    "                    print('D: ', D)\n",
    "                    print('gamma max: ', gamma_max)\n",
    "                    \n",
    "                # update d_prim\n",
    "                d_prim = d + gamma_max * D\n",
    "                \n",
    "                # update D_prim direction\n",
    "                D_prim[mu] = D[mu] + D[v]\n",
    "                D_prim[v] = 0\n",
    "                \n",
    "                ''''''\n",
    "                d_prim = self.l1_norm_on_sum(d_prim, 1)\n",
    "                D_prim = self.l1_norm_on_sum(D_prim, 0)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('d_prim: ', d_prim)\n",
    "                    print('D_prim: ', D_prim)\n",
    "                \n",
    "                # update J_prim\n",
    "                combined_kernel_matrix = self.get_weighted_kernel_matrix(d_prim)\n",
    "                J_prim, alpha = self.get_J(combined_kernel_matrix)\n",
    "\n",
    "                if verbose:\n",
    "                    print('J_prim: ', J_prim)\n",
    "                    print('alpha_prim: ', alpha)\n",
    "                    \n",
    "                # update J_d with inner updated d\n",
    "                # some use d_prim for J_d\n",
    "                combined_kernel_matrix = self.get_weighted_kernel_matrix(d)\n",
    "                J_d, alpha = self.get_J(combined_kernel_matrix)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('J_d: ', J_d)\n",
    "                    print('alpha: ', alpha)\n",
    "                    \n",
    "            # line search\n",
    "            gamma = self.armijo_rule(gamma_max, d, D, J_d)\n",
    "            \n",
    "            if verbose:\n",
    "                print('armijo gamma: ', gamma)\n",
    "                print('d: ', d)\n",
    "                print('D: ', D)\n",
    "            \n",
    "            # check d\n",
    "            flag = False\n",
    "            for i in d:\n",
    "                if i == 1:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag == False:\n",
    "                # update d\n",
    "                d = d + gamma * D\n",
    "                ''''''\n",
    "                d = self.l1_norm_on_sum(d, 1)\n",
    "            \n",
    "            if verbose:\n",
    "                print('final d: ', d)\n",
    "                \n",
    "        self.kernel_weights = np.copy(d)    \n",
    "        self.fitted_combined_kernel_matrix = self.get_weighted_kernel_matrix(d)\n",
    "        self.model_svm.fit(self.fitted_combined_kernel_matrix, self.y_train)\n",
    "        return self.kernel_weights\n",
    "        \n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        test_size = x_test.shape[0]\n",
    "        train_size = self.x_train.shape[0]\n",
    "        kernel_matrix = np.zeros((test_size, train_size))\n",
    "        for i in range(test_size):\n",
    "            for j in range(train_size):\n",
    "                kernel_matrix[i,j] = sum([self.kernel_weights[m] * self.kernel_functions[m](x_test[i, ], self.x_train[j,]) for m in range(len(self.kernel_functions))])\n",
    "        \n",
    "        self.predicted = np.array(self.model_svm.predict(kernel_matrix))\n",
    "        return self.predicted\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predicted_y = self.predict(X)\n",
    "        return float(sum(y == predicted_y))/len(y)\n",
    "    \n",
    "    \n",
    "    def matrix_pos_dif(self, matrix, etha):\n",
    "        # costraint: etha > 0\n",
    "        return np.all(np.linalg.eigvals(matrix) > etha)\n",
    "\n",
    "    \n",
    "    def get_kernel_matrices(self):\n",
    "        X = self.x_train\n",
    "        n = self.x_train.shape[0]\n",
    "        kernels = self.kernel_functions\n",
    "        M = len(kernels)\n",
    "        \n",
    "        kernel_matrices = [np.matrix(np.zeros((n,n))) for i in range(M)]\n",
    "        for m in range(M):\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    kernel_matrices[m][i,j] = kernels[m](X[i], X[j])\n",
    "        \n",
    "        \n",
    "        # kernel_matrices must be positive definite\n",
    "        # all eigenvalues greater than some Î· > 0\n",
    "        # to enforce a small ridge may be added to the diagonal of the Kernel matrices\n",
    "        constraint = [self.matrix_pos_dif(kernel_mat, 0) for kernel_mat in kernel_matrices]\n",
    "#         if self.verbose:\n",
    "#             print('pos dif: ', constraint)\n",
    "            \n",
    "        epsilon = 1e-2\n",
    "        for i, k_martix in enumerate(kernel_matrices):\n",
    "            if constraint[i] == False:\n",
    "                new_kernel = k_martix + epsilon* np.eye(k_martix.shape[0],k_martix.shape[1])\n",
    "                if self.matrix_pos_dif(new_kernel, 0) == False:\n",
    "                    pass\n",
    "#                     print('!!!!!!!!!!! Something wrong here!')\n",
    "                kernel_matrices[i] = new_kernel\n",
    "        \n",
    "        constraint = [self.matrix_pos_dif(kernel_mat, 0) for kernel_mat in kernel_matrices]\n",
    "#         if self.verbose:\n",
    "#             print('pos dif: ', constraint)\n",
    "            \n",
    "        # with using tuple we can't change it by mistake\n",
    "        # kernel_matrices ---> (m, n, n)\n",
    "        self.kernel_matrices = tuple(kernel_matrices)   \n",
    "        self.num_of_kernels = len(self.kernel_matrices)\n",
    "        \n",
    "        return self.kernel_matrices\n",
    "    \n",
    "    \n",
    "    def get_weighted_kernel_matrix(self, d):\n",
    "        M = self.num_of_kernels\n",
    "        n = self.kernel_matrices[0].shape[0]\n",
    "\n",
    "        # combined_kernel_matrix ---> (n,n)\n",
    "        combined_kernel_matrix = sum([d[m] * self.kernel_matrices[m] for m in range(M)])\n",
    "        return combined_kernel_matrix\n",
    "    \n",
    "    \n",
    "    def get_J(self, kernel_matrix):\n",
    "        if self.verbose:\n",
    "            print('enter J...')\n",
    "            \n",
    "        self.model_svm.fit(kernel_matrix, self.y_train)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('finish svm fit.')\n",
    "        \n",
    "        # geting alphas and thier indices in support vectors\n",
    "        # alpha ---> (num_support_vectors,)\n",
    "        alpha = self.model_svm.dual_coef_[0]\n",
    "        indices = self.model_svm.support_\n",
    "        \n",
    "        # all_alpha ---> (n,)\n",
    "        all_alpha = np.zeros((self.y_train.shape[0]))\n",
    "        n = all_alpha.shape[0]\n",
    "        for i in range(n):\n",
    "            if i in indices:  # we reach one of the support vectors\n",
    "                index = np.where(indices == i)\n",
    "                index = index[0][0]\n",
    "                # fill all_alpha\n",
    "                all_alpha[i] = alpha[index]\n",
    "            else:\n",
    "                continue  # it had been filled with zero\n",
    "                \n",
    "        # equation 10\n",
    "        # J ---> scaler (cost function)\n",
    "        # we should multiply pairwise because of sum_ij(alpa_i* alpha_i * y_i * y_j * kernel_ij)\n",
    "        J = -0.5*(np.absolute(all_alpha)@(kernel_matrix * self.y_matrix_outer)@(np.absolute(all_alpha.T)))\\\n",
    "               + np.sum(np.absolute(all_alpha))\n",
    "\n",
    "        # another calculation:\n",
    "        # alpha_matrix = np.outer(np.absolute(all_alpha),np.absolute(all_alpha))\n",
    "        # J = -0.5* (sum(sum(alpha_matrix * kernel_matrix * self.y_matrix_outer))) + np.sum(np.absolute(all_alpha))\n",
    "        \n",
    "        return J, all_alpha\n",
    "          \n",
    "        \n",
    "    def get_derivative_J(self, alpha):\n",
    "        # equation 11\n",
    "        M = self.num_of_kernels\n",
    "        \n",
    "        # dJ ---> (M,)\n",
    "        dJ = np.array([(-0.5 *(alpha@(self.kernel_matrices[m]*self.y_matrix_outer)@alpha.T)) for m in range(M)])\n",
    "        return dJ\n",
    "    \n",
    "    \n",
    "    def get_D_direction(self, d, dJ, mu):\n",
    "        # equation 12\n",
    "        \n",
    "        #TODO: is it nesscery to normlize dJ or D??\n",
    "        D = np.zeros(self.num_of_kernels)\n",
    "        D_mu = 0\n",
    "        for i in range(self.num_of_kernels):\n",
    "            if (d[i] < self.epsilon_d or d[i] == 0) and ((dJ[i] - dJ[mu]) > 0):\n",
    "                D[i] = 0.\n",
    "            elif i != mu:\n",
    "                D[i] = -dJ[i] + dJ[mu]\n",
    "                D_mu -= D[i]\n",
    "        D[mu] = D_mu\n",
    "        return D\n",
    "    \n",
    "    \n",
    "    def get_gamma_max_and_index(self, d, D):\n",
    "        gamma_max = None\n",
    "        v = None\n",
    "        for m in range(self.num_of_kernels):\n",
    "            if (gamma_max == None and D[m] < 0) or (D[m] < 0 and -d[m]/D[m] < gamma_max):\n",
    "                v = m\n",
    "                gamma_max = -d[m]/D[m]\n",
    "        return gamma_max, v\n",
    "    \n",
    "    \n",
    "    def armijo_rule(self, gamma_max, d, D, J_d, beta = 0.9, sigma = 0.01):\n",
    "        gamma = gamma_max\n",
    "        armijo_terminated = False\n",
    "        while not armijo_terminated:\n",
    "            new_d = d + gamma*D\n",
    "            \n",
    "            combined_kernel_matrix = self.get_weighted_kernel_matrix(new_d)\n",
    "            new_J_d, alpha = self.get_J(combined_kernel_matrix)\n",
    "            \n",
    "            new_dJ = self.get_derivative_J(alpha)\n",
    "            \n",
    "            if (J_d - new_J_d) >= sigma * np.sum(new_dJ * gamma * D):\n",
    "                # TODO: new_J_d <= J_d + gamma * armijo_sigma * D.T.dot(dJ)\n",
    "                armijo_terminated = True\n",
    "            else:\n",
    "                gamma *= beta\n",
    "        return gamma    \n",
    "   \n",
    "    \n",
    "    def get_duality_gap(self, J_d, alpha, dJ):      \n",
    "#         duality_gap = (J_d - np.sum(np.absolute(alpha)) + np.max(-dJ)) * 2\n",
    "        duality_gap = (J_d - np.sum(np.absolute(alpha)) + np.max(-dJ)) / J_d\n",
    "        return duality_gap[0,0]\n",
    "\n",
    "\n",
    "    def get_KKT_condition(self, iteration, dJ, d, epsilon = 0.01):\n",
    "        M = len(dJ)\n",
    "        if iteration == 0:\n",
    "            return False\n",
    "        else:\n",
    "            dJ_min = 1e+4\n",
    "            dJ_max = -1e+4\n",
    "            dm0_min = 1e+5\n",
    "            for m in range(M):\n",
    "                if d[m] > 0:\n",
    "                    if dJ[m] < dJ_min:\n",
    "                        dJ_min = dJ[m]\n",
    "\n",
    "                    if dJ[m] > dJ_max:\n",
    "                        dJ_max = dJ[m]\n",
    "                else:\n",
    "                    if dJ[m] < dm0_min:\n",
    "                        dm0_min = dJ[m]\n",
    "            result = (dJ_max - dJ_min < epsilon) and dm0_min >= dJ_max\n",
    "            return result\n",
    "\n",
    "        \n",
    "    def l1_norm_on_sum(self, input_list, sum_):\n",
    "        for i in range(len(input_list)):\n",
    "            input_list[i] = round(input_list[i], 8)\n",
    "\n",
    "        u = np.argmax(abs(input_list))\n",
    "        diff = sum_ - sum(input_list)\n",
    "        if diff <= 0.0001:\n",
    "            input_list[u] += diff\n",
    "            return input_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-damage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "dangerous-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:               \n",
    "    def load_datasets(self, dataset_path):\n",
    "        self.dataframe = pd.read_csv(dataset_path, sep=',', header=None)\n",
    "        index_label = self.dataframe.columns[-1]\n",
    "#         print(self.dataframe[index_label])\n",
    "#         self.dataframe = shuffle(self.dataframe)\n",
    "        \n",
    "        self.train_size = int(len(self.dataframe))\n",
    "        self.x_train = self.dataframe.copy()\n",
    "        self.x_train = self.x_train.drop([index_label], axis=1).to_numpy(dtype='float64')\n",
    "\n",
    "        self.y_train = self.dataframe[index_label]\n",
    "        self.y_train = self.y_train.to_numpy()\n",
    "        \n",
    "        print(\"Finished reading dataset ...\")\n",
    "        \n",
    "    def __init__(self, dataset_path):\n",
    "        self.load_datasets(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-jordan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "incorporated-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_1 = Kernel.gaussian(1.0)\n",
    "kernel_2 = Kernel.gaussian(0.5)\n",
    "kernel_3 = Kernel.gaussian(2.0)\n",
    "kernel_4 = Kernel.gaussian(0.1)\n",
    "kernel_5 = Kernel.linear()\n",
    "kernel_6 = Kernel.polynomial(2, 1)\n",
    "kernel_7 = Kernel.polynomial(3, 1)\n",
    "kernel_8 = Kernel.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "parallel-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Call_on_datasets():\n",
    "    def __init__(self, data_path_train, data_path_test, flag=False):\n",
    "        self.data_train = DataSet(dataset_path= data_path_train)\n",
    "        self.data_test = DataSet(dataset_path= data_path_test)\n",
    "\n",
    "        print('Dataset shape: ', self.data_train.x_train.shape, self.data_train.y_train.shape)\n",
    "#         print('Data Test: ', self.data_test.x_train.shape, self.data_test.y_train.shape)\n",
    "        \n",
    "        if flag:\n",
    "            sampling_size = self.data_train.x_train.shape[0]\n",
    "            tmp_y_train = self.data_test.y_train.reshape(-1, 1)\n",
    "\n",
    "            data_t = np.append(self.data_test.x_train, tmp_y_train, axis=1)\n",
    "            _data_train = sample(list(data_t), sampling_size)\n",
    "            _data_train = np.asarray(_data_train)\n",
    "            _y_train = _data_train[:, -1]\n",
    "            _x_train = np.delete(_data_train, -1, axis=1)\n",
    "            \n",
    "            self.data_test.x_train = _x_train\n",
    "            self.data_test.y_train = _y_train\n",
    "#             print('Data Test: ', self.data_test.x_train.shape, self.data_test.y_train.shape)\n",
    "\n",
    "\n",
    "    def evalue(self, list_kernels, verbose=True):\n",
    "        list_kernels = tuple(list_kernels)\n",
    "        if verbose:\n",
    "            print('\\nSimpleMKL..')\n",
    "        start = time.time()\n",
    "        simpleMkl_model = SimpleMkl(C=1., kernels=list_kernels)\n",
    "        simpleMkl_model.fit(self.data_train.x_train, self.data_train.y_train, verbose=False)\n",
    "        end = time.time()\n",
    "        processing_time = end - start\n",
    "        if verbose:\n",
    "            print('Running Time:  ', processing_time,'s')\n",
    "            print('weights: ', simpleMkl_model.kernel_weights)\n",
    "\n",
    "            print('\\n')\n",
    "            print('Train Accuracy: ')\n",
    "        start = time.time()\n",
    "        self.train_acc = simpleMkl_model.score(self.data_train.x_train, self.data_train.y_train)* 100\n",
    "        end = time.time()\n",
    "        processing_time = end - start\n",
    "        if verbose:\n",
    "            print(self.train_acc, '%')\n",
    "            print('Testing execution time: ', processing_time,'s')\n",
    "\n",
    "            print('\\n')\n",
    "            print('Test Accuracy: ')\n",
    "        start = time.time()\n",
    "        self.test_acc = simpleMkl_model.score(self.data_test.x_train, self.data_test.y_train)* 100\n",
    "        end = time.time()\n",
    "        processing_time = end - start\n",
    "        if verbose:\n",
    "            print(self.test_acc, '%')\n",
    "            print('Testing execution time: ', processing_time,'s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-madrid",
   "metadata": {},
   "source": [
    "# Breast-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "temporal-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (25, 10) (25,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   43.650038957595825 s\n",
      "weights:  [0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "44.0 %\n",
      "Testing execution time:  0.040164947509765625 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "64.0 %\n",
      "Testing execution time:  0.040081024169921875 s\n"
     ]
    }
   ],
   "source": [
    "breast_w = Call_on_datasets(data_path_train= './NewDatasets/new_breast_w_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_breast_w_test.data', flag=True)\n",
    "\n",
    "list_kernels = [kernel_8, kernel_1, kernel_2, kernel_3, kernel_4, kernel_5]   \n",
    "# list_kernels = [kernel_6, kernel_1, kernel_2]   \n",
    "\n",
    "breast_w.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-marketplace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fancy-biotechnology",
   "metadata": {},
   "source": [
    "# Messidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "arabic-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (40, 19) (40,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   1.349165916442871 s\n",
      "weights:  [1. 0. 0.]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "100.0 %\n",
      "Testing execution time:  0.11119771003723145 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "70.0 %\n",
      "Testing execution time:  0.16142964363098145 s\n"
     ]
    }
   ],
   "source": [
    "messidor = Call_on_datasets(data_path_train= './NewDatasets/new_messidor_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_messidor_test.data', flag=True)\n",
    "\n",
    "list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "# list_kernels = [kernel_6, kernel_1]   \n",
    "list_kernels = [kernel_6, kernel_1, kernel_2]   \n",
    "\n",
    "messidor.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-active",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "forward-ratio",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "blond-trial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (62, 6) (62,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   0.5553033351898193 s\n",
      "weights:  [0. 1. 0.]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "100.0 %\n",
      "Testing execution time:  0.36735081672668457 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "98.38709677419355 %\n",
      "Testing execution time:  0.2961091995239258 s\n"
     ]
    }
   ],
   "source": [
    "car = Call_on_datasets(data_path_train= './NewDatasets/new_car_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_car_test.data', flag=True)\n",
    "\n",
    "# list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "list_kernels = [kernel_8, kernel_1, kernel_2]   \n",
    "\n",
    "car.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "institutional-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (62, 6) (62,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   0.6819605827331543 s\n",
      "weights:  [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "100.0 %\n",
      "Testing execution time:  0.5845355987548828 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "91.93548387096774 %\n",
      "Testing execution time:  0.47055482864379883 s\n"
     ]
    }
   ],
   "source": [
    "car = Call_on_datasets(data_path_train= './NewDatasets/new_car_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_car_test.data', flag=True)\n",
    "\n",
    "list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "# list_kernels = [kernel_6, kernel_1, kernel_2]   \n",
    "\n",
    "car.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-entrance",
   "metadata": {},
   "source": [
    "# SpamBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "polyphonic-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (26, 57) (26,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   13.975008487701416 s\n",
      "weights:  [1. 0. 0.]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "96.15384615384616 %\n",
      "Testing execution time:  0.049661874771118164 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "88.46153846153845 %\n",
      "Testing execution time:  0.040589332580566406 s\n"
     ]
    }
   ],
   "source": [
    "spambase = Call_on_datasets(data_path_train= './NewDatasets/new_Spambase_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_Spambase_test.data', flag=True)\n",
    "\n",
    "list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "list_kernels = [kernel_5, kernel_1, kernel_2]   \n",
    "\n",
    "spambase.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-coordinator",
   "metadata": {},
   "source": [
    "# Coil2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "wooden-albania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   1.5594041347503662 s\n",
      "weights:  [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "95.23809523809523 %\n",
      "Testing execution time:  0.9004759788513184 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "96.42857142857143 %\n",
      "Testing execution time:  0.8556923866271973 s\n"
     ]
    }
   ],
   "source": [
    "coil = Call_on_datasets(data_path_train= './NewDatasets/new_coil2000_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_coil2000_test.data', flag=True)\n",
    "\n",
    "list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "# list_kernels = [kernel_5, kernel_6, kernel_2]   \n",
    "\n",
    "coil.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-internet",
   "metadata": {},
   "source": [
    "# Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "textile-block",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   0.06968379020690918 s\n",
      "weights:  [0.48609358 0.51390642]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "92.3076923076923 %\n",
      "Testing execution time:  0.017798900604248047 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "100.0 %\n",
      "Testing execution time:  0.017586469650268555 s\n"
     ]
    }
   ],
   "source": [
    "bank = Call_on_datasets(data_path_train= './NewDatasets/new_bank_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_bank_test.data', flag=True)\n",
    "\n",
    "# list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "# list_kernels = [kernel_5, kernel_1, kernel_2]\n",
    "list_kernels = [kernel_8, kernel_1]\n",
    "\n",
    "bank.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-exhibition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-income",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "automotive-martin",
   "metadata": {},
   "source": [
    "# Skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "useful-scholar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   0.040435791015625 s\n",
      "weights:  [0.31158425 0.19471775 0.10434062 0.19467869 0.19467869]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "100.0 %\n",
      "Testing execution time:  0.0974891185760498 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "100.0 %\n",
      "Testing execution time:  0.11562156677246094 s\n"
     ]
    }
   ],
   "source": [
    "skin = Call_on_datasets(data_path_train= './NewDatasets/new_skin_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_skin_test.data', flag=True)\n",
    "\n",
    "# list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "list_kernels = [kernel_3, kernel_1, kernel_8, kernel_2, kernel_4]   \n",
    "\n",
    "skin.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-universal",
   "metadata": {},
   "source": [
    "# covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "moral-alcohol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "\n",
      "SimpleMKL..\n",
      "Running Time:   0.02194070816040039 s\n",
      "weights:  [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      "\n",
      "\n",
      "Train Accuracy: \n",
      "100.0 %\n",
      "Testing execution time:  0.017953157424926758 s\n",
      "\n",
      "\n",
      "Test Accuracy: \n",
      "93.33333333333333 %\n",
      "Testing execution time:  0.017946958541870117 s\n"
     ]
    }
   ],
   "source": [
    "covtype = Call_on_datasets(data_path_train= './NewDatasets/new_covtype_train.data', \\\n",
    "                           data_path_test= './NewRandomSelectionDatasets_COVT/new_covtype_test_0.data', flag=True)\n",
    "\n",
    "list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4, kernel_5, kernel_6, kernel_7, kernel_8]   \n",
    "# list_kernels = [kernel_8, kernel_6, kernel_2]   \n",
    "\n",
    "covtype.evalue(list_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-consequence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-prague",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-beatles",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "protected-alabama",
   "metadata": {},
   "source": [
    "## compare with random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "large-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Datasets_final as Datasets_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cloudy-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSelection:\n",
    "    def __init__(self, dataset, sampling_size=25):\n",
    "        self.x_train = dataset.x_train\n",
    "        self.y_train = dataset.y_train\n",
    "        \n",
    "        data_t = np.append(self.x_train, self.y_train, axis=1)\n",
    "        \n",
    "        self.data_train = sample(list(data_t), sampling_size)\n",
    "        self.data_train = np.asarray(self.data_train)\n",
    "        self.y_train = self.data_train[:, -1]\n",
    "        self.x_train = np.delete(self.data_train, -1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ranging-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewDatasetHumanLabeling:\n",
    "    def __init__(self, dataset, new_x_train, output_dataset_path):\n",
    "        start = time.time()\n",
    "\n",
    "        new_y_train = self.LabelingNewDataset(dataset, new_x_train)\n",
    "        self.create_new_dataset_csv_file(new_x_train, new_y_train, output_dataset_path)\n",
    "\n",
    "        end = time.time()\n",
    "        self.processing_time = end - start\n",
    "    \n",
    "    def LabelingNewDataset(self, dataset, new_x_train):\n",
    "        new_y_train = []\n",
    "        i = 0\n",
    "        for new_x in new_x_train:\n",
    "            new_y_train.append(dataset.y_train[(dataset.x_train==new_x).all(axis=1).nonzero()[0][0]][0])\n",
    "        return new_y_train\n",
    "\n",
    "    def create_new_dataset_csv_file(self, new_x_train, new_y_train, output_dataset_path):\n",
    "        df = pd.DataFrame(new_x_train)\n",
    "        df[len(df.columns)] = new_y_train\n",
    "        # create a new csv file\n",
    "        df.to_csv(\"test.csv\", index=False)\n",
    "        \n",
    "        # remove first line of csv file which is the header of each coloumn\n",
    "        with open(\"test.csv\",'r') as f:\n",
    "            with open(output_dataset_path,'w') as f1:\n",
    "                next(f) # skip header line\n",
    "                for line in f:\n",
    "                    f1.write(line)\n",
    "        os.remove(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "improving-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_test(x_test, y_test, output_dataset_path):\n",
    "    df = pd.DataFrame(x_test)\n",
    "    df[len(df.columns)] = y_test\n",
    "    # create a new csv file\n",
    "    df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "    # remove first line of csv file which is the header of each coloumn\n",
    "    with open(\"test.csv\",'r') as f:\n",
    "        with open(output_dataset_path,'w') as f1:\n",
    "            next(f) # skip header line\n",
    "            for line in f:\n",
    "                f1.write(line)\n",
    "    os.remove(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-bracket",
   "metadata": {},
   "source": [
    "#  coil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "massive-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_n_algo():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        \n",
    "    def get_random_dataset(self, dataset, sampling_size):\n",
    "#         print(dataset.x_train.shape, dataset.y_train.shape)\n",
    "        random_selection = RandomSelection(dataset, sampling_size=sampling_size)\n",
    "#         print('random: ', random_selection.x_train.shape, random_selection.y_train.shape)\n",
    "        return random_selection\n",
    "        \n",
    "    def save_coil(self):\n",
    "        dataset = Datasets_F.Coil2000_Dataset(dataset_path='./Datasets/coil2000.dat',dataset_name='Coil2000', \n",
    "                            train_size=0.02, normalization_method='None')\n",
    "        random_selection = self.get_random_dataset(dataset, 68)\n",
    "        save_data_test(random_selection.x_train, random_selection.y_train, \"./NewRandomSelectionDatasets/new_coil2000_train.data\")\n",
    "        save_data_test(dataset.x_test, dataset.y_test, \"./NewRandomSelectionDatasets/new_coil2000_test.data\")\n",
    "        \n",
    "    def call_coil(self):\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        for i in range(15):\n",
    "            if self.method=='random':\n",
    "                self.save_coil()\n",
    "                coil = Call_on_datasets(data_path_train= \"./NewRandomSelectionDatasets/new_coil2000_train.data\", \\\n",
    "                                   data_path_test= \"./NewRandomSelectionDatasets/new_coil2000_test.data\", flag=True)\n",
    "            else: \n",
    "                coil = Call_on_datasets(data_path_train= './NewDatasets/new_coil2000_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_coil2000_test.data', flag=True)\n",
    "\n",
    "            list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4]   \n",
    "\n",
    "            coil.evalue(list_kernels, verbose=False)\n",
    "            self.train_acc.append(coil.train_acc)\n",
    "            self.test_acc.append(coil.test_acc)\n",
    "        print('all accuracies: ', self.test_acc)\n",
    "        print('min: ', min(self.test_acc))\n",
    "        print('max: ', max(self.test_acc))\n",
    "        print('mean: ', np.mean(self.test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "external-trigger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "Started reading dataset  Coil2000 ...\n",
      "Finished reading dataset  Coil2000 ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (68, 85) (68,)\n",
      "[92.64705882352942, 94.11764705882352, 95.58823529411765, 88.23529411764706, 94.11764705882352, 94.11764705882352, 92.64705882352942, 92.64705882352942, 95.58823529411765, 89.70588235294117, 89.70588235294117, 94.11764705882352, 92.64705882352942, 91.17647058823529, 91.17647058823529]\n",
      "min:  88.23529411764706\n",
      "max:  95.58823529411765\n",
      "mean:  92.54901960784315\n"
     ]
    }
   ],
   "source": [
    "random_method = run_n_algo('random')\n",
    "random_method.call_coil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "formed-castle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (84, 85) (84,)\n",
      "all accuracies:  [94.04761904761905, 92.85714285714286, 92.85714285714286, 97.61904761904762, 95.23809523809523, 94.04761904761905, 95.23809523809523, 95.23809523809523, 96.42857142857143, 97.61904761904762, 94.04761904761905, 95.23809523809523, 90.47619047619048, 94.04761904761905, 91.66666666666666]\n",
      "min:  90.47619047619048\n",
      "max:  97.61904761904762\n",
      "mean:  94.44444444444443\n"
     ]
    }
   ],
   "source": [
    "random_method = run_n_algo('selection')\n",
    "random_method.call_coil()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-present",
   "metadata": {},
   "source": [
    "# bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "outside-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_n_bank():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        \n",
    "    def get_random_dataset(self, dataset, sampling_size):\n",
    "#         print(dataset.x_train.shape, dataset.y_train.shape)\n",
    "        random_selection = RandomSelection(dataset, sampling_size=sampling_size)\n",
    "#         print('random: ', random_selection.x_train.shape, random_selection.y_train.shape)\n",
    "        return random_selection\n",
    "        \n",
    "    def save_bank(self):\n",
    "        dataset = Datasets_F.Bank_Marketing_Dataset('./Datasets/bank-full.csv', \"Bank Marketing\", 'y', \n",
    "                                      train_size=0.1, normalization_method=\"None\", balance=True)\n",
    "        random_selection = self.get_random_dataset(dataset, 13)\n",
    "        save_data_test(random_selection.x_train, random_selection.y_train, \"./NewRandomSelectionDatasets/new_bank_train.data\")\n",
    "        save_data_test(dataset.x_test, dataset.y_test, \"./NewRandomSelectionDatasets/new_bank_test.data\")\n",
    "        \n",
    "    def call_bank(self):\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        for i in range(15):\n",
    "            if self.method=='random':\n",
    "                self.save_bank()\n",
    "                bank = Call_on_datasets(data_path_train= \"./NewRandomSelectionDatasets/new_bank_train.data\", \\\n",
    "                                   data_path_test= \"./NewRandomSelectionDatasets/new_bank_test.data\", flag=True)\n",
    "            else: \n",
    "                bank = Call_on_datasets(data_path_train= './NewDatasets/new_bank_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_bank_test.data', flag=True)\n",
    "\n",
    "            list_kernels = [kernel_8, kernel_1]\n",
    "\n",
    "            bank.evalue(list_kernels, verbose=False)\n",
    "            self.train_acc.append(bank.train_acc)\n",
    "            self.test_acc.append(bank.test_acc)\n",
    "        print('all accuracies: ', self.test_acc)\n",
    "        print('min: ', min(self.test_acc))\n",
    "        print('max: ', max(self.test_acc))\n",
    "        print('mean: ', np.mean(self.test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ethical-cooperative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Started reading dataset  Bank Marketing ...\n",
      "Finished reading dataset  Bank Marketing ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "all accuracies:  [100.0, 92.3076923076923, 84.61538461538461, 0.0, 84.61538461538461, 46.15384615384615, 15.384615384615385, 7.6923076923076925, 7.6923076923076925, 92.3076923076923, 23.076923076923077, 0.0, 92.3076923076923, 7.6923076923076925, 15.384615384615385]\n",
      "min:  0.0\n",
      "max:  100.0\n",
      "mean:  44.61538461538461\n"
     ]
    }
   ],
   "source": [
    "random_method = run_n_bank('random')\n",
    "random_method.call_bank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "drawn-amino",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (13, 16) (13,)\n",
      "all accuracies:  [84.61538461538461, 84.61538461538461, 100.0, 92.3076923076923, 100.0, 100.0, 92.3076923076923, 84.61538461538461, 100.0, 92.3076923076923, 92.3076923076923, 92.3076923076923, 76.92307692307693, 76.92307692307693, 92.3076923076923]\n",
      "min:  76.92307692307693\n",
      "max:  100.0\n",
      "mean:  90.76923076923076\n"
     ]
    }
   ],
   "source": [
    "select_method = run_n_bank('selection')\n",
    "select_method.call_bank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-hello",
   "metadata": {},
   "source": [
    "# skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "identical-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_n_skin():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        \n",
    "    def get_random_dataset(self, dataset, sampling_size):\n",
    "#         print(dataset.x_train.shape, dataset.y_train.shape)\n",
    "        random_selection = RandomSelection(dataset, sampling_size=sampling_size)\n",
    "#         print('random: ', random_selection.x_train.shape, random_selection.y_train.shape)\n",
    "        return random_selection\n",
    "        \n",
    "    def save_skin(self):\n",
    "        dataset = Datasets_F.Skin_NonSkin_Dataset('./Datasets/Skin_NonSkin.txt', \"Skin Segmentation\",\n",
    "                                             train_size=0.0005, normalization_method=\"None\")\n",
    "        random_selection = self.get_random_dataset(dataset, 18)\n",
    "        save_data_test(random_selection.x_train, random_selection.y_train, \"./NewRandomSelectionDatasets/new_skin_train.data\")\n",
    "        save_data_test(dataset.x_test, dataset.y_test, \"./NewRandomSelectionDatasets/new_skin_test.data\")\n",
    "        \n",
    "    def call_skin(self):\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        for i in range(15):\n",
    "            if self.method=='random':\n",
    "                self.save_skin()\n",
    "                skin = Call_on_datasets(data_path_train= \"./NewRandomSelectionDatasets/new_skin_train.data\", \\\n",
    "                                   data_path_test= \"./NewRandomSelectionDatasets/new_skin_test.data\", flag=True)\n",
    "            else: \n",
    "                skin = Call_on_datasets(data_path_train= './NewDatasets/new_skin_train.data', \\\n",
    "                           data_path_test= './NewDatasets/new_skin_test.data', flag=True)\n",
    "\n",
    "            list_kernels = [kernel_3, kernel_1, kernel_8, kernel_2, kernel_4]   \n",
    "\n",
    "            skin.evalue(list_kernels, verbose=False)\n",
    "            self.train_acc.append(skin.train_acc)\n",
    "            self.test_acc.append(skin.test_acc)\n",
    "        print('all accuracies: ', self.test_acc)\n",
    "        print('min: ', min(self.test_acc))\n",
    "        print('max: ', max(self.test_acc))\n",
    "        print('mean: ', np.mean(self.test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "mexican-seventh",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Started reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset  Skin Segmentation ...\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "all accuracies:  [72.22222222222221, 83.33333333333334, 66.66666666666666, 94.44444444444444, 83.33333333333334, 83.33333333333334, 94.44444444444444, 94.44444444444444, 72.22222222222221, 77.77777777777779, 88.88888888888889, 83.33333333333334, 72.22222222222221, 77.77777777777779, 83.33333333333334]\n",
      "min:  66.66666666666666\n",
      "max:  94.44444444444444\n",
      "mean:  81.85185185185186\n"
     ]
    }
   ],
   "source": [
    "random_method = run_n_skin('random')\n",
    "random_method.call_skin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "challenging-somewhere",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "all accuracies:  [77.77777777777779, 88.88888888888889, 77.77777777777779, 83.33333333333334, 83.33333333333334, 77.77777777777779, 83.33333333333334, 83.33333333333334, 77.77777777777779, 72.22222222222221, 83.33333333333334, 83.33333333333334, 94.44444444444444, 77.77777777777779, 94.44444444444444]\n",
      "min:  72.22222222222221\n",
      "max:  94.44444444444444\n",
      "mean:  82.5925925925926\n"
     ]
    }
   ],
   "source": [
    "select_method = run_n_skin('selection')\n",
    "select_method.call_skin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "nervous-plane",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (18, 3) (18,)\n",
      "all accuracies:  [83.33333333333334, 100.0, 83.33333333333334, 66.66666666666666, 83.33333333333334, 77.77777777777779, 83.33333333333334, 88.88888888888889, 61.111111111111114, 94.44444444444444, 77.77777777777779, 83.33333333333334, 83.33333333333334, 83.33333333333334, 83.33333333333334]\n",
      "min:  61.111111111111114\n",
      "max:  100.0\n",
      "mean:  82.22222222222221\n"
     ]
    }
   ],
   "source": [
    "select_method = run_n_skin('selection')\n",
    "select_method.call_skin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-belle",
   "metadata": {},
   "source": [
    "# covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "operational-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_n_covt():\n",
    "    def __init__(self, method):\n",
    "        self.method = method\n",
    "        \n",
    "    def get_random_dataset(self, dataset, sampling_size):\n",
    "#         print(dataset.x_train.shape, dataset.y_train.shape)\n",
    "        random_selection = RandomSelection(dataset, sampling_size=sampling_size)\n",
    "#         print('random: ', random_selection.x_train.shape, random_selection.y_train.shape)\n",
    "        return random_selection\n",
    "        \n",
    "    def save_covt(self, iteration):\n",
    "        dataset = Datasets_F.Covertype_Dataset('./Datasets/covtype.data', \"Covertype\", \n",
    "                                               train_size=0.02, normalization_method=\"None\")\n",
    "        random_selection = self.get_random_dataset(dataset, 50)\n",
    "        save_data_test(random_selection.x_train, random_selection.y_train, \"./NewRandomSelectionDatasets/new_covtype_train_\"+str(iteration)+\".data\")\n",
    "        save_data_test(dataset.x_test, dataset.y_test, \"./NewRandomSelectionDatasets/new_covtype_test_\"+str(iteration)+\".data\")\n",
    "        \n",
    "    def call_covt(self):\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        for i in range(15):\n",
    "            if self.method=='random':\n",
    "#                 self.save_covt(i)\n",
    "                covt = Call_on_datasets(data_path_train= \"./NewRandomSelectionDatasets_COVT/new_covtype_train_\"+str(i)+\".data\", \\\n",
    "                                   data_path_test= \"./NewRandomSelectionDatasets_COVT/new_covtype_test_\"+str(i)+\".data\", flag=True)\n",
    "            else: \n",
    "                covt = Call_on_datasets(data_path_train= './NewDatasets/new_covtype_train.data', \\\n",
    "                           data_path_test= './NewRandomSelectionDatasets_COVT/new_covtype_test_0.data', flag=True)\n",
    "\n",
    "#             list_kernels = [kernel_3, kernel_1, kernel_8, kernel_2, kernel_4]   \n",
    "            list_kernels = [kernel_1, kernel_2, kernel_3, kernel_4, kernel_5, kernel_6, kernel_7, kernel_8]   \n",
    "\n",
    "            covt.evalue(list_kernels, verbose=False)\n",
    "            self.train_acc.append(covt.train_acc)\n",
    "            self.test_acc.append(covt.test_acc)\n",
    "        print('all accuracies: ', self.test_acc)\n",
    "        print('min: ', min(self.test_acc))\n",
    "        print('max: ', max(self.test_acc))\n",
    "        print('mean: ', np.mean(self.test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "dimensional-interaction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (50, 54) (50,)\n",
      "all accuracies:  [84.0, 70.0, 52.0, 54.0, 76.0, 80.0, 57.99999999999999, 64.0, 48.0, 52.0, 68.0, 74.0, 52.0, 54.0, 60.0]\n",
      "min:  48.0\n",
      "max:  84.0\n",
      "mean:  63.06666666666667\n"
     ]
    }
   ],
   "source": [
    "random_method = run_n_covt('random')\n",
    "random_method.call_covt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "czech-stadium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "Finished reading dataset ...\n",
      "Finished reading dataset ...\n",
      "Dataset shape:  (15, 54) (15,)\n",
      "all accuracies:  [80.0, 93.33333333333333, 93.33333333333333, 86.66666666666667, 80.0, 73.33333333333333, 86.66666666666667, 93.33333333333333, 66.66666666666666, 80.0, 80.0, 73.33333333333333, 93.33333333333333, 86.66666666666667, 93.33333333333333]\n",
      "min:  66.66666666666666\n",
      "max:  93.33333333333333\n",
      "mean:  84.0\n"
     ]
    }
   ],
   "source": [
    "select_method = run_n_covt('selection')\n",
    "select_method.call_covt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-graham",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-session",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
